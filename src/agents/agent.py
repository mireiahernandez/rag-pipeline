from abc import ABC, abstractmethod
from typeguard import typechecked
import os
from mistralai import Mistral
from mistralai.models import (
    UserMessage, ToolMessage, SystemMessage, AssistantMessage,
    ChatCompletionResponse
)
from src.retrievers.retriever_pipeline import BaseRetrieverPipeline
from typing import List, Any, Tuple, Union, Optional
import json
from src.agents.tools import get_default_tools
import logging


# Set up logger
logger = logging.getLogger(__name__)

logger.setLevel(logging.INFO)


class LLMError(Exception):
    """Base exception for LLM-related errors"""
    pass


class NoResponseError(LLMError):
    """Raised when no response is received from the LLM"""
    pass


class EmptyResponseError(LLMError):
    """Raised when the LLM response is empty or invalid"""
    pass


# Define a type for valid message types
MessageType = Union[AssistantMessage, SystemMessage, ToolMessage, UserMessage]


@typechecked
class Agent(ABC):
    @abstractmethod
    async def chat(self, query: str) -> str:
        pass


@typechecked
class SimpleAgent(Agent):
    def __init__(
        self,
        mistral_api_key: str = "",
        model: str = "mistral-large-latest"
    ):
        self.mistral_api_key = mistral_api_key or os.getenv("MISTRAL_API_KEY")
        if not self.mistral_api_key:
            raise ValueError("Mistral API key must be provided")
        self.client = Mistral(api_key=self.mistral_api_key)
        self.model = model

    async def chat(self, query: str) -> str:
        messages: List[MessageType] = [UserMessage(role="user", content=query)]
        try:
            response: Optional[ChatCompletionResponse] = \
                await self.client.chat.complete_async(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000
                )

            if not response:
                raise NoResponseError("Failed to get response from the model")

            if not response.choices or not response.choices[0].message:
                raise EmptyResponseError("No response generated by the model")

            content = response.choices[0].message.content
            if not isinstance(content, str):
                raise EmptyResponseError("Response content is not a string")

            return content or ""

        except Exception as e:
            raise LLMError(f"Error in LLM call: {str(e)}") from e


@typechecked
class RAGAgent(Agent):
    def __init__(
        self,
        retriever_pipeline: BaseRetrieverPipeline,
        mistral_api_key: str = "",
        model: str = "mistral-large-latest"
    ):
        self.mistral_api_key = mistral_api_key or os.getenv("MISTRAL_API_KEY")
        if not self.mistral_api_key:
            raise ValueError("Mistral API key must be provided")
        self.client = Mistral(api_key=self.mistral_api_key)
        self.model = model
        self.retriever_pipeline = retriever_pipeline
        self.tools = get_default_tools()
        logger.info(f"Initialized RAGAgent with model: {model}")

    async def _call_tool(self, tool_name: str, **kwargs: Any) -> Any:
        logger.info(f"Calling tool: {tool_name} with args: {kwargs}")
        if tool_name == "query_knowledge_base":
            result = await self._query_knowledge_base(
                kwargs["rewritten_query"])
            logger.info(f"Tool {tool_name} returned: {result}")
            return result
        raise ValueError(f"Unknown tool: {tool_name}")

    async def _query_knowledge_base(self, query: str) -> List[str]:
        logger.info(f"Querying knowledge base with: {query}")
        docs: List[str] = await self.retriever_pipeline.retrieve(query)
        logger.info(f"Retrieved {len(docs)} documents")
        return docs

    async def _get_llm_response(
        self,
        messages: List[MessageType],
        use_tools: bool = True
    ) -> Tuple[str, List[Any]]:
        logger.info("Getting LLM response")
        logger.info(f"Messages: {messages}")
        try:
            response: Optional[ChatCompletionResponse] = \
                await self.client.chat.complete_async(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000,
                    tools=self.tools if use_tools else None,
                    tool_choice="auto" if use_tools else None,
                    response_format=None,
                )

            if not response:
                logger.error("No response received from LLM")
                raise NoResponseError("Failed to get response from the model")

            if not response.choices or not response.choices[0].message:
                logger.error("Empty response from LLM")
                raise EmptyResponseError("No response generated by the model")

            content = response.choices[0].message.content or ""
            logger.info(f"LLM response content: {content}")

            if not isinstance(content, str):
                logger.error(f"Invalid response type: {type(content)}")
                raise EmptyResponseError("Response content is not a string")

            tool_calls = getattr(
                response.choices[0].message, 'tool_calls', []) or []
            if tool_calls:
                logger.info(f"Tool calls requested: {tool_calls}")

            return content, tool_calls

        except Exception as e:
            logger.error(f"Error in LLM call: {str(e)}", exc_info=True)
            raise LLMError(f"Error in LLM call: {str(e)}") from e

    async def chat(self, query: str) -> str:
        logger.info(f"Starting chat with query: {query}")
        system_prompt = (
            "You are an intelligent, agentic assistant designed to help the user with their questions. "  # noqa: E501
            "Use the `query_knowledge_base` tool to retrieve relevant information. "  # noqa: E501
            "Do not ask clarifying questions to the user, but instead make multiple calls to the knowledge base."  # noqa: E501
            "You can make multiple calls to the knowledge base if the query involves multiple sub-queries or requires complex information."  # noqa: E501
        )

        messages: List[MessageType] = [
            SystemMessage(content=system_prompt),
            UserMessage(content=query)
        ]

        counter = 0
        while counter < 5:
            logger.info(f"Processing iteration {counter + 1}")
            # Get LLM response and potential tool calls
            response_text, tool_calls = await self._get_llm_response(messages)
            # If no tool calls, we're done
            if not tool_calls:
                logger.info(
                    "No tool calls requested, returning final response"
                )
                return response_text

            # Execute all tool calls
            results = []
            for tool_call in tool_calls:
                logger.info(f"Executing tool call: {tool_call}")
                args = json.loads(tool_call.function.arguments)
                result = await self._call_tool(tool_call.function.name, **args)
                if isinstance(result, list):
                    results.extend(result)
                else:
                    results.append(result)
                logger.info(f"Tool call results: {results}")

            # Add assistant's response and tool results to messages
            messages.extend([
                SystemMessage(content=response_text),
                UserMessage(
                    content=f"Tool call results: {' '.join(results)}. "
                    "Please continue with your analysis or provide a final answer."  # noqa: E501
                )
            ])
            logger.info("Added tool results to conversation")

            counter += 1

        logger.info("Reached maximum iterations, returning final response")
        return response_text
