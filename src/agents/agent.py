from abc import ABC, abstractmethod
from typeguard import typechecked
import os
from mistralai import Mistral
from mistralai.models import (
    UserMessage, ToolMessage, SystemMessage, AssistantMessage,
    ChatCompletionResponse
)
from src.retrievers.retriever_pipeline import BaseRetrieverPipeline
from typing import List, Any, Tuple, Union, Optional
import json
from src.agents.tools import get_default_tools


class LLMError(Exception):
    """Base exception for LLM-related errors"""
    pass


class NoResponseError(LLMError):
    """Raised when no response is received from the LLM"""
    pass


class EmptyResponseError(LLMError):
    """Raised when the LLM response is empty or invalid"""
    pass


# Define a type for valid message types
MessageType = Union[AssistantMessage, SystemMessage, ToolMessage, UserMessage]


@typechecked
class Agent(ABC):
    @abstractmethod
    async def chat(self, query: str) -> str:
        pass


@typechecked
class SimpleAgent(Agent):
    def __init__(
        self,
        mistral_api_key: str = "",
        model: str = "mistral-large-latest"
    ):
        self.mistral_api_key = mistral_api_key or os.getenv("MISTRAL_API_KEY")
        if not self.mistral_api_key:
            raise ValueError("Mistral API key must be provided")
        self.client = Mistral(api_key=self.mistral_api_key)
        self.model = model

    async def chat(self, query: str) -> str:
        messages: List[MessageType] = [UserMessage(role="user", content=query)]
        try:
            response: Optional[ChatCompletionResponse] = \
                await self.client.chat.complete_async(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000
                )

            if not response:
                raise NoResponseError("Failed to get response from the model")

            if not response.choices or not response.choices[0].message:
                raise EmptyResponseError("No response generated by the model")

            content = response.choices[0].message.content
            if not isinstance(content, str):
                raise EmptyResponseError("Response content is not a string")

            return content or ""

        except Exception as e:
            raise LLMError(f"Error in LLM call: {str(e)}") from e


@typechecked
class RAGAgent(Agent):
    def __init__(
        self,
        retriever_pipeline: BaseRetrieverPipeline,
        mistral_api_key: str = "",
        model: str = "mistral-large-latest"
    ):
        self.mistral_api_key = mistral_api_key or os.getenv("MISTRAL_API_KEY")
        if not self.mistral_api_key:
            raise ValueError("Mistral API key must be provided")
        self.client = Mistral(api_key=self.mistral_api_key)
        self.model = model
        self.retriever_pipeline = retriever_pipeline
        self.tools = get_default_tools()

    async def _call_tool(self, tool_name: str, **kwargs: Any) -> Any:
        if tool_name == "query_knowledge_base":
            return await self._query_knowledge_base(kwargs["query"])
        raise ValueError(f"Unknown tool: {tool_name}")

    async def _query_knowledge_base(self, query: str) -> List[str]:
        docs: List[str] = await self.retriever_pipeline.retrieve(query)
        return docs

    async def _get_llm_response(
        self,
        messages: List[MessageType],
        use_tools: bool = True
    ) -> Tuple[str, List[Any]]:
        """Get LLM response and tool calls if any"""
        try:
            # Call complete_async with explicit arguments
            response: Optional[ChatCompletionResponse] = \
                await self.client.chat.complete_async(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000,
                    tools=self.tools if use_tools else None,
                    tool_choice="auto" if use_tools else None,
                    response_format=None,
                )

            if not response:
                raise NoResponseError("Failed to get response from the model")

            if not response.choices or not response.choices[0].message:
                raise EmptyResponseError("No response generated by the model")

            content = response.choices[0].message.content or ""

            if not isinstance(content, str):
                raise EmptyResponseError("Response content is not a string")

            tool_calls = getattr(
                response.choices[0].message, 'tool_calls', []) or []

            return content, tool_calls

        except Exception as e:
            raise LLMError(f"Error in LLM call: {str(e)}") from e

    async def chat(self, query: str) -> str:
        system_prompt = (
            "You are an intelligent, agentic assistant designed to provide accurate and helpful responses by leveraging a knowledge base. "  # noqa: E501
            "When answering user queries, use the `query_knowledge_base` tool to dynamically retrieve relevant information. "  # noqa: E501
            "You can make multiple calls to the knowledge base if the query involves multiple sub-queries or requires complex information. "  # noqa: E501
            "Only use the knowledge base for questions you do not know the answer to."  # noqa: E501
        )

        messages: List[MessageType] = [
            SystemMessage(content=system_prompt),
            UserMessage(content=query)
        ]

        counter = 0
        while counter < 5:
            # Get LLM response and potential tool calls
            response_text, tool_calls = await self._get_llm_response(messages)

            # If no tool calls, we're done
            if not tool_calls:
                return response_text

            # Execute all tool calls
            results = []
            for tool_call in tool_calls:
                args = json.loads(tool_call.function.arguments)
                result = await self._call_tool(tool_call.function.name, **args)
                if isinstance(result, list):
                    results.extend(result)
                else:
                    results.append(result)

            # Add assistant's response and tool results to messages
            messages.extend([
                SystemMessage(content=response_text),
                UserMessage(
                    content=f"Tool call results: {' '.join(results)}. "
                    "Please continue with your analysis or provide a final answer."  # noqa: E501
                )
            ])

            counter += 1

        return response_text
